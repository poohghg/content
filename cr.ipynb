{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import time\n",
    "\n",
    "class unBiasedDice:\n",
    "    def __init__(self):\n",
    "        self.output_val = 0\n",
    "        self.input_val = 0\n",
    "        self.justice = 0\n",
    "\n",
    "    def main(self, input_val, percent):\n",
    "        self.input_val = input_val\n",
    "        self.percent = percent\n",
    "        self.stone = [1,2,3,4,5,6]\n",
    "        self.stone.remove(input_val)\n",
    "        self.rollingStone = []\n",
    "        \n",
    "        for i in range(1,10):\n",
    "            self.stone.extend(self.stone)\n",
    "            \n",
    "        if (percent < 100 and percent >= 0):\n",
    "            self.justice = int((len(self.stone) * self.percent) / (100 - self.percent))        \n",
    "        elif(percent >= 100):\n",
    "            self.justice = 1\n",
    "            self.stone = [input_val]\n",
    "        \n",
    "        if (self.input_val == 0):\n",
    "            self.output_val = randint(1,6)\n",
    "            print(self.output_val)\n",
    "        else:\n",
    "            for i in range(1,self.justice):\n",
    "                self.rollingStone.append(self.input_val)\n",
    "            self.stone.extend(self.rollingStone)\n",
    "            print_val = self.stone[randint(0,len(self.stone)-1)]\n",
    "                    \n",
    "if __name__ == \"__main__\":\n",
    "    f = unBiasedDice()\n",
    "    f.main(2, 22)\n",
    "            \n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 한빛 네트워크 출판사 크롤링\n",
    "\n",
    "!pip install selenium\n",
    "!pip install bs4\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from tkinter import *\n",
    "from tkinter.filedialog import *\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "#from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "def Get_mok():\n",
    "    data = []\n",
    "    s1 = driver.page_source\n",
    "    s2 = BeautifulSoup(s1, \"html.parser\")\n",
    "    s3 = s2.find(\"div\", id = \"tabs_3\")\n",
    "    s3 = s3.text\n",
    "    print(s3)\n",
    "    driver.back()\n",
    "\n",
    "def FilePath():\n",
    "    global path1\n",
    "    path1 = askopenfilename()\n",
    "    print(path1)\n",
    "\n",
    "def runDriver(url):\n",
    "    global driver\n",
    "    driver = webdriver.Chrome(path1)\n",
    "    #driver = webdriver.PhantomJS(path1)\n",
    "    driver.get(url)\n",
    "    \n",
    "def Rolling(aa):\n",
    "    driver.find_element_by_xpath('//*[@id=\"container\"]/div[2]/div[2]/li[' +aa+ ']/div/span/img').click()\n",
    "    time.sleep(2)\n",
    "\n",
    "def Pager(bb):\n",
    "    driver.get('http://www.hanbit.co.kr/store/books/new_book_list.html?page='+bb+'&brand=&cate1=&cate2=&searchKey=&keyWord=')\n",
    "    time.sleep(2)\n",
    "\n",
    "\n",
    "FilePath()\n",
    "runDriver('http://www.hanbit.co.kr/store/books/new_book_list.html')\n",
    "\n",
    "for pages in range(1, 10000):\n",
    "    print(\"======================\")\n",
    "    print(pages)\n",
    "    print(\"======================\")\n",
    "    for roller in range(1, 21):\n",
    "        roller = str(roller)\n",
    "        Rolling(roller)\n",
    "        Get_mok()\n",
    "    pages = pages + 1\n",
    "    pages = str(pages)    \n",
    "    Pager(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 환율 크롤링\n",
    "#####\n",
    "\n",
    "\n",
    "from urllib.request import Request, urlopen\n",
    "from operator import eq\n",
    "import json\n",
    "import time\n",
    "import logging.handlers\n",
    "import multiprocessing\n",
    "from multiprocessing import Process, Queue\n",
    "import threading\n",
    "from threading import Thread\n",
    "\n",
    "\n",
    "class Agent2:\n",
    "    def __init__(self):\n",
    "        self.USDKRW = 0\n",
    "        self.KRWEUR = 0\n",
    "        self.EURCNY = 0\n",
    "        self.GBPJPY = 0\n",
    "        self.KRWCNY = 0\n",
    "        self.EURKRW = 0\n",
    "        self.USDKRW = 0\n",
    "        self.USDKRW = 0\n",
    "        \n",
    "        self.t1 = 0\n",
    "        \n",
    "    def main(self):\n",
    "        self.t1 = time.time()\n",
    "        \n",
    "        while True:\n",
    "            self.Monitor = 0\n",
    "            self.workerSum = 0\n",
    "            self.workerList = []\n",
    "            self.Warning = 0\n",
    "            self.switching = 0\n",
    "            self.hashRate = 0\n",
    "            \n",
    "            Url = \"http://earthquake.kr/exchange\"\n",
    "            Req = Request(Url,headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                \n",
    "            self.Webpage = urlopen(Req).read() \n",
    "            jsonToPython = json.loads(self.Webpage)\n",
    "            \n",
    "            self.USDKRW = (jsonToPython['USDKRW'][0])\n",
    "            self.KRWJPY = (jsonToPython['KRWJPY'][0])\n",
    "            self.KRWEUR = (jsonToPython['KRWEUR'][0])\n",
    "            self.EURCNY = (jsonToPython['EURCNY'][0])\n",
    "            self.GBPJPY = (jsonToPython['GBPJPY'][0])\n",
    "            self.KRWCNY = (jsonToPython['KRWCNY'][0])\n",
    "            self.EURKRW = (jsonToPython['EURKRW'][0])\n",
    "\n",
    "            self.t2 = time.time()\n",
    "            \n",
    "            print(\">>> [  USDKRW ]  [  KRWJPY ]  [  KRWEUR ]  [  EURCNY ]  [  GBPJPY ]  [  KRWCNY ]  [  EURKRW ]\")\n",
    "            \n",
    "            print(\">>> [%9s]  [%9s]  [%9s]  [%9s]  [%9s]  [%9s]  [%9s]\" \\\n",
    "                         % (self.USDKRW, self.KRWJPY, self.KRWEUR, self.EURCNY, self.GBPJPY, self.KRWCNY, self.EURKRW))\n",
    "            \n",
    "\n",
    "            timer = self.t2 - self.t1\n",
    "            if (timer < 5):\n",
    "                time.sleep(5 - timer)\n",
    "            else:\n",
    "                time.sleep(timer)\n",
    "            \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    agent = Agent2()\n",
    "\n",
    "    Thread(target = agent.main()).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "from tkinter.filedialog import *\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "##############윈도우 컨트롤################\n",
    "root = Tk()\n",
    "\n",
    "root.configure(width = \"130m\", height = \"166m\")\n",
    "\n",
    "###############전역변수 저장###############\n",
    "list = []\n",
    "shop_list = []\n",
    "select = \"\"\n",
    "index = 0\n",
    "#################함수부###################\n",
    "\n",
    "def FilePath():\n",
    "    global path1\n",
    "    path1 = askopenfilename()\n",
    "    print(path1)\n",
    "\n",
    "def runDriver():\n",
    "    global driver\n",
    "    driver = webdriver.Chrome(path1)\n",
    "    driver.get(\"http://used.aladin.co.kr/usedstore/wgate.aspx\")\n",
    "    \n",
    "def searchQuery():\n",
    "    for i in list:\n",
    "        username = driver.find_element_by_xpath(\"//*[@id=\\\"SearchWord\\\"]\")\n",
    "        # //*[@id=\"SearchWord\"]\n",
    "        # 검색 XPATH\n",
    "        username.send_keys(\"\\b\"*50)\n",
    "        username.send_keys(i)\n",
    "        driver.find_element_by_xpath(\"//*[@id=\\\"global_search\\\"]/input\").click()\n",
    "        # //*[@id=\"global_search\"]/input\n",
    "        # 검색창을 찾은후 검색버튼을 클릭\n",
    "        s1 = driver.page_source\n",
    "        s2 = BeautifulSoup(s1, \"html.parser\")\n",
    "        s3 = s2.find(\"div\", class_ = \"ss_book_box\")\n",
    "        s4=s3.find(\"div\", class_=\"usedshop_off_text2_box\")\n",
    "        \n",
    "        shop_list.append(str(s4.text))\n",
    "        print(shop_list) \n",
    "        \n",
    "def getBookNames():\n",
    "    for i in range(len(list)):\n",
    "        resultBox.insert(i+1, list[i])\n",
    "    \n",
    "    \n",
    "def getQuery():\n",
    "    global text1\n",
    "    list.append(text1.get())\n",
    "    print(list)\n",
    "    text1 = Entry(root)\n",
    "    text1.place(x= 30, y = 150, width = 250, height = 30)\n",
    "\n",
    "def getAddress():\n",
    "    global addressBox\n",
    "    addressBox = Listbox(root)\n",
    "    addressBox.place(x = 265, y = 300, width = 175, height = 200)    \n",
    "    address2 = shop_list[index].split(\", \")\n",
    "    for i in range(len(address2)):\n",
    "        addressBox.insert(i+1, address2[i])\n",
    "    \n",
    "def onselect(evt):\n",
    "    global index\n",
    "    w = evt.widget\n",
    "    index = int(w.curselection()[0])\n",
    "    value = w.get(index)\n",
    "    print(index)\n",
    "    \n",
    "\n",
    "\n",
    "##############부품배치####################\n",
    "text1 = Entry(root)\n",
    "text1.place(x= 30, y = 150, width = 250, height = 30)\n",
    "resultBox = Listbox(root)\n",
    "resultBox.place(x = 30, y = 300, width = 175, height = 200)\n",
    "resultBox.bind('<<ListboxSelect>>', onselect)\n",
    "addressBox = Listbox(root)\n",
    "addressBox.place(x = 265, y = 300, width = 175, height = 200)\n",
    "\n",
    "Button(root, text=\"파일 가져 오기\", command = FilePath).place(\n",
    "    x = 30, y = 30, width = 150, height = 40)\n",
    "Button(root, text=\"크롬드라이버 실행\", command = runDriver).place(\n",
    "    x = 30, y = 90, width = 150, height = 40)\n",
    "Button(root, text=\"검색어 입력\", command = getQuery).place(\n",
    "    x = 320, y = 150, width = 120, height = 30)\n",
    "Button(root, text=\"검색 실행\", command = searchQuery).place(\n",
    "   x = 210 , y = 30, width = 150, height = 40)  \n",
    "Button(root, text=\"결과 조회\", command = getBookNames).place(\n",
    "   x = 210 , y = 90, width = 150, height = 40)  \n",
    "Button(root, text=\"->\", command = getAddress).place(\n",
    "   x = 210 , y = 380, width = 50, height = 40)  \n",
    "\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium\n",
    "!pip install bs4\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from tkinter import *\n",
    "from tkinter.filedialog import *\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "#from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "def Get_mok():\n",
    "    data = []\n",
    "    s1 = driver.page_source\n",
    "    s2 = BeautifulSoup(s1, \"html.parser\")\n",
    "    #s3 = s2.find(\"div\",id=\"tabs_3\")\n",
    "    s3 = s2.find(\"div\",class_= \"detail_conbox hanbit_edit_view\")\n",
    "    print(s3)\n",
    "    data.append(str(s3.text))\n",
    "    print(data)\n",
    "    driver.back()\n",
    "def FilePath():\n",
    "    global path1\n",
    "    path1 = askopenfilename()\n",
    "    print(path1)\n",
    "\n",
    "def runDriver(url):\n",
    "    global driver\n",
    "    driver = webdriver.Chrome(path1)\n",
    "    #driver = webdriver.PhantomJS(path1)\n",
    "    driver.get(url)\n",
    "    \n",
    "def Rolling(aa):\n",
    "    driver.find_element_by_xpath('//*[@id=\"container\"]/div[2]/div[2]/li[' +aa+ ']/div/span/img').click()\n",
    "    \n",
    "    time.sleep(2)\n",
    "\n",
    "def Pager(bb):\n",
    "    driver.get('http://www.hanbit.co.kr/store/books/new_book_list.html?page='+bb+'&brand=&cate1=&cate2=&searchKey=&keyWord=')\n",
    "    time.sleep(2)\n",
    "\n",
    "FilePath()\n",
    "runDriver('http://www.hanbit.co.kr/store/books/new_book_list.html')\n",
    "\n",
    "for pages in range(1, 10000):\n",
    "    print(\"======================\")\n",
    "    print(pages)\n",
    "    print(\"======================\")\n",
    "    for roller in range(1, 21):\n",
    "        roller = str(roller)\n",
    "        Rolling(roller)\n",
    "        Get_mok()\n",
    "    pages = pages + 1\n",
    "    pages = str(pages)    \n",
    "    Pager(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#위키북스 크롤링\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "\n",
    "def dynamic_get_page(url):\n",
    "    try:\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('headless')\n",
    "        \"\"\"\n",
    "        Headless라는 용어는 ‘창이 없는’과 같다고 이해하시면 됩니다. \n",
    "       브라우저(크롬 등)을 이용해 인터넷을 브라우징 할 때 기본적으로 창이 뜨고 HTML파일을 불러오고, \n",
    "        CSS파일을 불러와 어떤 내용을 화면에 그러야 할지 계산을 하는 작업을 브라우저가 자동으로 진행해줍니다.\n",
    "        \n",
    "        \"\"\"\n",
    "        options.add_argument(\"disable-gpu\")\n",
    "        driver = webdriver.Chrome(executable_path='C:/driver/chromedriver.exe', chrome_options=options)\n",
    "        driver.get(url)\n",
    "        driver.execute_script(\"document.getElementById('load-more').style.width = '100%';\")\n",
    "        # 더보기 \n",
    "        count = 0\n",
    "        print(\"Scraping book list...\")\n",
    "        # 더는 자료가 없습니다 문구가 뜰때까지 더보기를 스크립\n",
    "        # getelementbyid는 더보기박스임\n",
    "        while driver.find_element_by_id(\"load-more\").text != \"더는 자료가 없습니다.\":\n",
    "            driver.find_element_by_css_selector(\"#load-more\").click()\n",
    "            count += 1\n",
    "            time.sleep(random.randint(1, 3))\n",
    "\n",
    "            print(\"Searching more books: {}\".format(count))\n",
    "        print(\"All book list saved!\")\n",
    "\n",
    "        html = driver.page_source\n",
    "        driver.quit()\n",
    "        return html\n",
    "\n",
    "    except:\n",
    "        driver.quit()\n",
    "        return '0'\n",
    "\n",
    "def scrape_list_page(html_text):\n",
    "    soup = BeautifulSoup(html_text ,\"lxml\")\n",
    "    for li in soup.find('ul', {'id':'front-book-list'}).find_all('li', {'class':'book-in-front'}):\n",
    "        url = li.find('a').get('href')\n",
    "        yield url\n",
    "        # 각 url 저장\n",
    "        # 스크립트할 페이지의 각 url저장\n",
    "        #<a href=\"http://wikibook.co.kr/aws-design-migration/\">\n",
    "        \n",
    "def scrape_detail_page(url):\n",
    "    session = requests.Session()\n",
    "    response = session.get(url).text\n",
    "\n",
    "    root = BeautifulSoup(response, \"lxml\")\n",
    "    normalize_space = lambda string: re.sub(r'\\s+', ' ', string).strip()\n",
    "    # 띄어쓰기 값들을 공백으로\n",
    "    ##질문 파싱해온 페이지를 정리\n",
    "    # 파싱해온 url 값의 내용을\n",
    "    bookInfo = {\n",
    "        'url': url,\n",
    "        'title': root.find(\"div\", {'id':'content'}).find(\"h1\", {'class':'main-title'}).text,\n",
    "        'price': root.find(\"div\", {'id':'content'}).find(\"ul\", {'class':'book-info'}).find_all(\"li\")[4].text.split('|')[0].strip().replace('원', '').replace(',', ''),\n",
    "        'content': [normalize_space(p.text) for p in root.find(\"div\", {'id':'toc'}).find('ul').find_all('li') if normalize_space(p.text)]\n",
    "    }\n",
    "    # 첫번째 | 값을 분리 한후 공백을 없엔후 원 과 , 를 공백으로 대체\n",
    "    return bookInfo\n",
    "\n",
    "def json_save(filename, data):\n",
    "    with open(filename+\".json\", \"w\", encoding=\"UTF-8-sig\") as f:\n",
    "        json.dump(data, fp=f, ensure_ascii=False)\n",
    "    print(\"JSON data saved - filename: \" + filename)\n",
    "\n",
    "def develop_data(html):\n",
    "    print(\"Scraping book info...\")\n",
    "    book_list = dict()\n",
    "    index = 0\n",
    "    for url in scrape_list_page(html):    # 각 url 을 for문으로 추출\n",
    "        bookInfo = scrape_detail_page(url)    # 각 url의내용을 저장\n",
    "        book_list[str(index)] = bookInfo\n",
    "        index += 1\n",
    "        print(\"# of scraped book: {}\".format(index))\n",
    "        time.sleep(random.randint(1, 3))\n",
    "    print(\"All book info saved!\")\n",
    "        \n",
    "    print(book_list)\n",
    "    json_save(\"wikibook_books\", book_list)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    html = dynamic_get_page(\"http://wikibook.co.kr/\")\n",
    "    try:\n",
    "        int(html)\n",
    "        print('No data found')\n",
    "    except: develop_data(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from tkinter import *\n",
    "from tkinter.filedialog import *\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import sys\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import ConnectionFailure\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "#from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "def Get_mok():\n",
    "    s1 = driver.page_source\n",
    "    s2 = BeautifulSoup(s1,\"html.parser\")\n",
    "    s3 = s2.find(\"div\",class_ = \"store_product_info_box\")\n",
    "    s4 = s3.find(\"h3\").text.strip()\n",
    "    s5 = s3.find(\"ul\", class_=\"info_list\")\n",
    "    s6 = s5.find_all(\"li\")[0]\n",
    "    s7 = s6.find(\"span\").text.strip()\n",
    "    s8 = s2.find(\"label\",class_=\"payment_box curr\")\n",
    "    s9 = s2.find(\"span\",class_=\"pbr\").text.strip()\n",
    "    data = {'title':[s4],'author':[s7],'cost':[s9]}\n",
    "    driver.back()\n",
    "    \n",
    "    try:\n",
    "        c = MongoClient('localhost', 27017)\n",
    "        print (\"Connected successfully\")\n",
    "        \n",
    "    except (ConnectionFailure, e):\n",
    "        sys.stderr.write(\"Could not connect to MongoDB: %s\" % e)\n",
    "        sys.exit(1)\n",
    "\n",
    "    mongoose = c.test\n",
    "    mongoose_c = mongoose.newbbbljk\n",
    "    mongoose_c.insert_one(data)\n",
    "    docs = mongoose_c.find()\n",
    "    \n",
    "    \n",
    "    for i in docs:\n",
    "        print(i)\n",
    "        \n",
    "    \n",
    "def FilePath():\n",
    "    global path1\n",
    "    path1 = askopenfilename()\n",
    "    print(path1)\n",
    "\n",
    "def runDriver(url):\n",
    "    global driver\n",
    "    driver = webdriver.Chrome(path1)\n",
    "    #driver = webdriver.PhantomJS(path1)\n",
    "    driver.get(url)\n",
    "    \n",
    "def Rolling(aa):\n",
    "    driver.find_element_by_xpath('//*[@id=\"container\"]/div[2]/div[2]/li[' +aa+ ']/div/span/img').click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "def Pager(bb):\n",
    "    driver.get('http://www.hanbit.co.kr/store/books/new_book_list.html?page='+bb+'&brand=&cate1=&cate2=&searchKey=&keyWord=')\n",
    "    time.sleep(2)\n",
    "\n",
    "\n",
    "FilePath()\n",
    "runDriver('http://www.hanbit.co.kr/store/books/new_book_list.html')\n",
    "\n",
    "for pages in range(1, 10000):\n",
    "    print(\"======================\")\n",
    "    print(pages)\n",
    "    print(\"======================\")\n",
    "    for roller in range(1, 21):\n",
    "        roller = str(roller)\n",
    "        Rolling(roller)\n",
    "        time.sleep(0.5)\n",
    "        Get_mok()\n",
    "    pages = pages + 1\n",
    "    pages = str(pages)    \n",
    "    Pager(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
